{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet détection Covid par radio : Etape de modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation du modèle LeNet à faire tourner avec Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import tqdm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import pickle\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn import metrics\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 images dans le dataset\n"
     ]
    }
   ],
   "source": [
    "#1 : Indiquer le dossier et compter le nb d'images dans le dataset\n",
    "dossier_train = \"\"\n",
    "data_dir = pathlib.Path(dossier_train).with_suffix('')\n",
    "image_count = len(list(data_dir.glob('*/*.png')))\n",
    "print(f\"{image_count} images dans le dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 4 classes.\n",
      "Using 1600 files for training.\n",
      "Found 2000 files belonging to 4 classes.\n",
      "Using 400 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#2 : Chargement du dataset dans Keras\n",
    "#define parameters for loader\n",
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "\n",
    "#Create train set 80%\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  color_mode = \"grayscale\")\n",
    "\n",
    "#Create validation set = 20%\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  color_mode = \"grayscale\")\n",
    "\n",
    "#Class names = from subfolders in Train folder\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure dataset for performance with CACHE (not loading images at each epoch) and PREFETCH (overlapping data processing and model execution)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction de l'algorithme LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implémentation du CNN Le Net5\n",
    "# Architecture du modèle\n",
    "inputs=Input(shape = (256, 256 ,1), name = \"Input\")\n",
    "\n",
    "lenet = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 1)),\n",
    "    layers.Conv2D(filters = 30,                     # Nombre de filtres\n",
    "                kernel_size = (5, 5),            # Dimensions du noyau\n",
    "                padding = 'valid',               # Mode de Dépassement\n",
    "                input_shape = (28, 28, 1),       # Dimensions de l'image en entrée\n",
    "                activation = 'relu', \n",
    "                name = \"Conv1\"),             # Fonction d'activation\n",
    "    layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "    layers.Conv2D(filters = 16,                    \n",
    "                kernel_size = (3, 3),          \n",
    "                padding = 'valid',             \n",
    "                activation = 'relu',\n",
    "                name = \"Conv2\"),\n",
    "    layers.MaxPooling2D(pool_size = (2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(rate = 0.2),\n",
    "    layers.Dense(units = 128,\n",
    "                activation = 'relu',\n",
    "                name = \"Dense1\"),\n",
    "    layers.Dense(units = 10,\n",
    "                activation = 'softmax',\n",
    "                name = \"Dense_final\")],\n",
    "        name = \"LeNet\")\n",
    "\n",
    "# Compilation\n",
    "lenet.compile(loss='categorical_crossentropy',  # fonction de perte\n",
    "              optimizer='adam',                 # algorithme de descente de gradient\n",
    "              metrics=['accuracy'])             # métrique d'évaluation\n",
    "\n",
    "\n",
    "#Résumé\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définir les callbacks : Earlystopping, Tensorboard\n",
    "\n",
    "#Early stopping\n",
    "Early_Stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience = 10,\n",
    "    verbose = 1,\n",
    "    restore_best_weights = True)\n",
    "\n",
    "#Tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement_1\n",
    "#Entraînement du modèle\n",
    "\n",
    "model = lenet #update when changing models\n",
    "\n",
    "#Train model\n",
    "epochs=100\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  callbacks = [Early_Stopping, tensorboard_callback]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize training results\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thiba\\Documents\\Projets data\\OCT23_BDS_Radios_Poumons\\notebooks\\drafts\\3-Thibaut-Modélisation_3_LeNet.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiba/Documents/Projets%20data/OCT23_BDS_Radios_Poumons/notebooks/drafts/3-Thibaut-Mod%C3%A9lisation_3_LeNet.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dossier_sauv \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../../models/Thibaut/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiba/Documents/Projets%20data/OCT23_BDS_Radios_Poumons/notebooks/drafts/3-Thibaut-Mod%C3%A9lisation_3_LeNet.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m nom_modèle \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mModèle_dense_1.keras\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thiba/Documents/Projets%20data/OCT23_BDS_Radios_Poumons/notebooks/drafts/3-Thibaut-Mod%C3%A9lisation_3_LeNet.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39msave(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dossier_sauv, nom_modèle))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Sauvegarde du modèle\n",
    "model = lenet\n",
    "#Sauvegarde du modèle\n",
    "dossier_sauv = \"../../models/Thibaut/\"\n",
    "nom_modèle = f\"{model._name}.-valacc{val_acc[-1]:.2f}.keras\"\n",
    "model.save(os.path.join(dossier_sauv, nom_modèle))  # The file needs to end with the .keras extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export results to dict & csv\n",
    "#Nom modèle : model._name, date: , dataset folder, dataset size, ephocs : len(val_acc), best_model : train_acc, val_acc, train_loss, val_loss\n",
    "#datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#Dump to a pickle file\n",
    "# Pickle the history to file\n",
    "filepath = f\"{model._name}_history.pkl\"\n",
    "with open(filepath, 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = f\"{model._name}_history.csv\"\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
