{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template notebook pour charger et évaluer un modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Importer les librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thiba\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import tqdm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import pickle\n",
    "from tensorflow.keras import datasets, layers, models, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Importer le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ModeleDense1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_3 (Rescaling)     (None, 256, 256, 1)       0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " Dense2 (Dense)              (None, 128)               8388736   \n",
      "                                                                 \n",
      " Dense5 (Dense)              (None, 32)                4128      \n",
      "                                                                 \n",
      " Dense6 (Dense)              (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8392996 (32.02 MB)\n",
      "Trainable params: 8392996 (32.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('../../models/Thibaut/Modèle_dense_1.keras')\n",
    "\n",
    "#charger les résultats enregistrés\n",
    "with open('file_name.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "# Show the model architecture\n",
    "model.summary()\n",
    "\n",
    "model = models.load_model('file.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Importer les jeux d'entraînement et validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "        Keras doesn't provide any more advanced feature than just taking a fraction of your training data for validation. If you need something more advanced, like stratified sampling to make sure classes are well represented in the sample, then you need to do this manually outside of Keras (using say, scikit-learn or numpy) and then pass that validation data to keras through the validation_data parameter in model.fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 4 classes.\n",
      "Using 1600 files for training.\n",
      "Found 2000 files belonging to 4 classes.\n",
      "Using 400 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#1 : Import du dataset et compte des images\n",
    "dossier_train = \"../../Data/Train_val_set_1/\"\n",
    "data_dir = pathlib.Path(dossier_train).with_suffix('')\n",
    "image_count = len(list(data_dir.glob('*/*.png')))\n",
    "print(f\"{image_count} images dans le dataset\")\n",
    "\n",
    "#2 : Chargement du dataset dans Keras\n",
    "#define parameters for loader\n",
    "batch_size = 128\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "\n",
    "#Create train set 80%\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  color_mode = \"grayscale\",\n",
    "  shuffle = True)\n",
    "\n",
    "#Create validation set 20%\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  color_mode = \"grayscale\",\n",
    "  shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Evaluer le modèle sur un jeu de validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
