{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet radiologie bootcamp DST : Etape 2, Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de preprocessing : \n",
    "* Ouvrir chaque image avec son masque associé\n",
    "* Convertir en Grayscale\n",
    "* Redimensionner les images à la taille des masques (256 * 256)\n",
    "* Appliquer les masques\n",
    "* Equilibrer le nouveau dataset\n",
    "* Séparer le dataset en ensemble d'entraînement (70%), de validation (15%) et de test (15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des bibliothèques\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os, pathlib\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil #pour copier des fichiers vers les dossiers de preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 1 : création des images masquées et en échelles de gris\n",
    "* Ouvrir chaque image en grayscale en itérant sur les dossiers\n",
    "* Ouvrir le masque associé en grayscale\n",
    "* Resizer l'image en 256*256\n",
    "* Appliquer le masque\n",
    "* Enregistrer l'image dans un nouveau dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sous-fonction qui ouvre une image à partir du filepath, la convertit en Grayscale et la redimensionne en 256*256\n",
    "#En input : le filepath d'une image dans le dossier \"image\" => plutôt retravailler le masque\n",
    "def load_and_resize(file):\n",
    "    img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (256, 256))\n",
    "    return img_resized\n",
    "\n",
    "#Sous-fonction qui va chercher le masque et le convertit en Grayscale\n",
    "#En input : le filepath d'une image dans le dossier \"image\" qui est l'input de la fonction finale\n",
    "def load_mask(file):\n",
    "    mask_path = file.replace(\"images\",\"masks\")\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return mask\n",
    "\n",
    "#Fonction qui applique le masque et enregistre l'image ainsi préprocessée dans un nouveau dossier\n",
    "#Input : image et masque (données d'origine), nom du fichier et chemin du dossier cible pour l'enregistrement\n",
    "def get_masked_imgs(sourceimg, nom_cible, dossier_cible):\n",
    "    img_resized = load_and_resize(sourceimg)\n",
    "    mask = load_mask(sourceimg)\n",
    "    img_masked = cv2.bitwise_and(img_resized, mask)\n",
    "    cv2.imwrite(os.path.join(dossier_cible, nom_cible), img_masked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test sur 1 fichier\n",
    "fichier = \"C:\\\\Users\\\\thiba\\\\Documents\\\\Projets data\\\\Projet-radio-DST_TG\\\\data\\\\Viral Pneumonia\\\\images\\\\Viral Pneumonia-14.png\"\n",
    "nom_cible = os.path.split(fichier)[1]\n",
    "dossier_cible = \"..\\\\data\\\\Preprocessing_1\"\n",
    "\n",
    "get_masked_imgs(fichier, nom_cible, dossier_cible)\n",
    "\n",
    "\n",
    "\n",
    "#print(img_resized.shape)\n",
    "#print(mask.shape)\n",
    "#plt.figure(figsize=(15, 5))\n",
    "#plt.subplot(131)\n",
    "#plt.imshow(img_resized, cmap = \"Greys_r\")\n",
    "#plt.subplot(132)\n",
    "#plt.imshow(mask, cmap = \"Greys_r\")\n",
    "#plt.subplot(133)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#Définition des dossiers d'origine et cible, et lancement des fonctions => voir le script python dans SRC\n",
    "\n",
    "#dossiers_source_dict = {\"COVID\" : \"../data/COVID/images/\", \"NORMAL\" : \"../data/Normal/images/\", \"VIRAL_PNEUMONIA\" : \"../data/Viral Pneumonia/images/\", \"LUNG_OPACITY\" : \"../data/Lung_Opacity/images/\" }\n",
    "dossiers_source_dict = {\"TEST\" : \"../data/test/images/\"}\n",
    "dossier_cible = \"..\\\\data\\\\Preprocessing_1\"\n",
    "\n",
    "#Itérer sur les dossiers pour récupérer les fichiers images \n",
    "for n,filepath in dossiers_source_dict.items():\n",
    "    for f in tqdm(os.listdir(filepath)):\n",
    "        nom_cible = f\n",
    "        file = os.path.join(filepath, f)\n",
    "        get_masked_imgs(file, nom_cible, dossier_cible)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2 : Equilibrage des classes par sous-échantillonage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import de la base excel metadata\n",
    "meta_Covid = pd.read_excel(\"../data/COVID.metadata.xlsx\")\n",
    "meta_Covid.insert(loc = 0, column=\"CLASS\", value =  \"COVID\")\n",
    "meta_Normal = pd.read_excel(\"../data/Normal.metadata.xlsx\")\n",
    "meta_Normal.insert(loc = 0, column=\"CLASS\", value =  \"NORMAL\")\n",
    "meta_VPneu = pd.read_excel(\"../data/Viral Pneumonia.metadata.xlsx\")\n",
    "meta_VPneu.insert(loc = 0, column=\"CLASS\", value =  \"VIRAL_PNEUMONIA\")\n",
    "meta_LO = pd.read_excel(\"../data/Lung_Opacity.metadata.xlsx\")\n",
    "meta_LO.insert(loc = 0, column=\"CLASS\", value =  \"LUNG_OPACITY\")\n",
    "#Fusion des 4 metafichiers dans un dataframe\n",
    "metadata = pd.concat([meta_Covid, meta_LO, meta_Normal, meta_VPneu], axis = 0)\n",
    "#Enregistrement en csv\n",
    "metadata.to_csv(\"../data/metadata_compil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "COVID              1345\n",
       "LUNG_OPACITY       1345\n",
       "NORMAL             1345\n",
       "VIRAL_PNEUMONIA    1345\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nb de valeurs par classe dans metadata\n",
    "print(len(metadata))\n",
    "metadata[\"CLASS\"].value_counts()\n",
    "\n",
    "#Utilisation d'un RandomUnderSampler pour équilibrer le dataset\n",
    "ru = RandomUnderSampler(random_state = 123, replacement = False)\n",
    "meta_ru, y_ru = ru.fit_resample(metadata, metadata[\"CLASS\"])\n",
    "meta_ru.head()\n",
    "meta_ru[\"CLASS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5380"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_ru[\"CLASS\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta Train :\n",
      " VIRAL_PNEUMONIA    942\n",
      "COVID              942\n",
      "NORMAL             941\n",
      "LUNG_OPACITY       941\n",
      "Name: CLASS, dtype: int64\n",
      "meta VAL :\n",
      " LUNG_OPACITY       202\n",
      "VIRAL_PNEUMONIA    202\n",
      "NORMAL             202\n",
      "COVID              201\n",
      "Name: CLASS, dtype: int64\n",
      "meta TEST :\n",
      " NORMAL             202\n",
      "COVID              202\n",
      "LUNG_OPACITY       202\n",
      "VIRAL_PNEUMONIA    201\n",
      "Name: CLASS, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TRAIN    3766\n",
       "VAL       807\n",
       "TEST      807\n",
       "Name: SET, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sélection aléatoire et enregistrement des fichiers dans les dossiers TRAIN (70%), VALIDATE (15%), TEST (15%)\n",
    "# Séparation jeu de TRAIN à 70/30\n",
    "meta_TRAIN, meta_TESTVAL, y1, y2 = train_test_split(meta_ru, y_ru, test_size = 0.3, random_state = 123, stratify = y_ru)\n",
    "\n",
    "#Séparation du jeu de VALIDATION et TEST à 50/50\n",
    "meta_VAL, meta_TEST, y3, y4 = train_test_split(meta_TESTVAL, y2, test_size = 0.5, random_state = 123, stratify = y2)\n",
    "\n",
    "print(\"meta Train :\\n\", meta_TRAIN[\"CLASS\"].value_counts())\n",
    "print(\"meta VAL :\\n\", meta_VAL[\"CLASS\"].value_counts())\n",
    "print(\"meta TEST :\\n\", meta_TEST[\"CLASS\"].value_counts())\n",
    "meta_TRAIN.head()\n",
    "\n",
    "meta_TRAIN[\"SET\"] = \"TRAIN\"\n",
    "meta_VAL[\"SET\"] = \"VAL\"\n",
    "meta_TEST[\"SET\"] = \"TEST\"\n",
    "meta_dataset = pd.concat([meta_TRAIN, meta_VAL, meta_TEST], axis = 0)\n",
    "meta_dataset[\"SET\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3 : enregistrement des 3 datasets dans 3 sous-dossiers TRAIN, VAL, TEST du dossier PREPROCESSING_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "      <th>FILE NAME</th>\n",
       "      <th>FORMAT</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>URL</th>\n",
       "      <th>SET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>VIRAL_PNEUMONIA</td>\n",
       "      <td>Viral Pneumonia-46</td>\n",
       "      <td>PNG</td>\n",
       "      <td>256*256</td>\n",
       "      <td>https://www.kaggle.com/paultimothymooney/chest...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>COVID</td>\n",
       "      <td>COVID-2461</td>\n",
       "      <td>PNG</td>\n",
       "      <td>256*256</td>\n",
       "      <td>https://bimcv.cipf.es/bimcv-projects/bimcv-cov...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>VIRAL_PNEUMONIA</td>\n",
       "      <td>Viral Pneumonia-210</td>\n",
       "      <td>PNG</td>\n",
       "      <td>256*256</td>\n",
       "      <td>https://www.kaggle.com/paultimothymooney/chest...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>VIRAL_PNEUMONIA</td>\n",
       "      <td>Viral Pneumonia-1272</td>\n",
       "      <td>PNG</td>\n",
       "      <td>256*256</td>\n",
       "      <td>https://www.kaggle.com/paultimothymooney/chest...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>COVID</td>\n",
       "      <td>COVID-1693</td>\n",
       "      <td>PNG</td>\n",
       "      <td>256*256</td>\n",
       "      <td>https://bimcv.cipf.es/bimcv-projects/bimcv-cov...</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                CLASS             FILE NAME FORMAT     SIZE  \\\n",
       "4080  VIRAL_PNEUMONIA    Viral Pneumonia-46    PNG  256*256   \n",
       "1106            COVID            COVID-2461    PNG  256*256   \n",
       "4244  VIRAL_PNEUMONIA   Viral Pneumonia-210    PNG  256*256   \n",
       "5306  VIRAL_PNEUMONIA  Viral Pneumonia-1272    PNG  256*256   \n",
       "1058            COVID            COVID-1693    PNG  256*256   \n",
       "\n",
       "                                                    URL    SET  \n",
       "4080  https://www.kaggle.com/paultimothymooney/chest...  TRAIN  \n",
       "1106  https://bimcv.cipf.es/bimcv-projects/bimcv-cov...  TRAIN  \n",
       "4244  https://www.kaggle.com/paultimothymooney/chest...  TRAIN  \n",
       "5306  https://www.kaggle.com/paultimothymooney/chest...  TRAIN  \n",
       "1058  https://bimcv.cipf.es/bimcv-projects/bimcv-cov...  TRAIN  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5380it [01:45, 50.92it/s]\n"
     ]
    }
   ],
   "source": [
    "dossier_source = \"..\\\\data\\\\Preprocessing_1\"\n",
    "dossiers_cibles_dict = {\"TRAIN\" : \"..\\\\data\\\\Preprocessing_2\\Train\", \"VAL\" : \"..\\\\data\\\\Preprocessing_2\\Train\", \"TEST\" : \"..\\\\data\\\\Preprocessing_2\\Train\"}\n",
    "\n",
    "#Fonction pour copier les fichiers dans le dossier cible\n",
    "def copy_to_folder(filename, source_folder, target_folder):\n",
    "    filename = filename + \".png\"\n",
    "    #chemin d'accès au fichier soruce\n",
    "    src = os.path.join(source_folder, filename)\n",
    "    #chemin d'accès du fichier copié\n",
    "    dst = os.path.join(target_folder, filename)\n",
    "    #copie\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "#Itération sur les noms de fichiers retenus par le TRAIN TEST SPLIT\n",
    "for index, row in tqdm(meta_dataset.iterrows()):\n",
    "    dossier_cible = dossiers_cibles_dict[row[\"SET\"]]\n",
    "    copy_to_folder(row[\"FILE NAME\"], dossier_source, dossier_cible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du jeu de donnée préprocessé et équilibré"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principe : \n",
    "- à partir du jeu d'images préprocessé (Préprocessing_1)\n",
    "- On part du nombre minimal d'échantillons par classe : 1345 (5380 images au total)\n",
    "- On met de côté 15% des images pour le test (pas fourni au modèle) : 800 (200 par classe), avec équilibre des classes\n",
    "- On copie 4 jeux :\n",
    "    - 400 images (100*4)\n",
    "    - 1000 images (250 * 4)\n",
    "    - 2000 images (500 * 4)\n",
    "    - 4000 images (1000 * 4) \n",
    "    - Toutes les images : 4576 (1144/classe), arrondi\n",
    "\n",
    "Noms : Train_set_400, Train_set_1000, Train_set_2000, Train_set_4000, Train_set_4576, Test_set_800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORMAL             10192\n",
       "LUNG_OPACITY        6012\n",
       "COVID               3616\n",
       "VIRAL_PNEUMONIA     1345\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nombre d'échantillons de la classe la moins représentée : 1345\n",
    "metadata = pd.read_csv(\"../data/metadata_compil.csv\")\n",
    "metadata.head()\n",
    "metadata[\"CLASS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Impossible de créer un fichier déjà existant: 'c:\\\\Users\\\\thiba\\\\Documents\\\\Projets data\\\\data\\\\Train_set_400'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thiba\\Documents\\Projets data\\OCT23_BDS_Radios_Poumons\\notebooks\\2.0 Thibaut Preprocessing.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiba/Documents/Projets%20data/OCT23_BDS_Radios_Poumons/notebooks/2.0%20Thibaut%20Preprocessing.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m dossier \u001b[39min\u001b[39;00m dossiers_a_creer:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiba/Documents/Projets%20data/OCT23_BDS_Radios_Poumons/notebooks/2.0%20Thibaut%20Preprocessing.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     new_dossier \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(dossier_racine), dossier)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thiba/Documents/Projets%20data/OCT23_BDS_Radios_Poumons/notebooks/2.0%20Thibaut%20Preprocessing.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(new_dossier)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiba/Documents/Projets%20data/OCT23_BDS_Radios_Poumons/notebooks/2.0%20Thibaut%20Preprocessing.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m sousdossier \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mNORMAL\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCOVID\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLUNG OPACITY\u001b[39m\u001b[39m\"\u001b[39m , \u001b[39m\"\u001b[39m\u001b[39mVIRAL PNEUMONIA\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thiba/Documents/Projets%20data/OCT23_BDS_Radios_Poumons/notebooks/2.0%20Thibaut%20Preprocessing.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         new_sousdossier \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(new_dossier, sousdossier)\n",
      "File \u001b[1;32mc:\\Users\\thiba\\anaconda3\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[0;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Impossible de créer un fichier déjà existant: 'c:\\\\Users\\\\thiba\\\\Documents\\\\Projets data\\\\data\\\\Train_set_400'"
     ]
    }
   ],
   "source": [
    "#Créer les dossiers cibles #a marché mais ils n'apparaissent pas dans Windows... a la mano !\n",
    "dossier_racine = \"../../data/\"\n",
    "dossiers_a_creer = [\"Train_set_400\", \"Train_set_1000\", \"Train_set_2000\", \"Train_set_4000\", \"Train_set_4580\", \"Test_set_800\"]\n",
    "for dossier in dossiers_a_creer:\n",
    "    new_dossier = os.path.join(os.path.abspath(dossier_racine), dossier)\n",
    "    os.makedirs(new_dossier)\n",
    "    for sousdossier in [\"NORMAL\", \"COVID\", \"LUNG OPACITY\" , \"VIRAL PNEUMONIA\"]:\n",
    "        new_sousdossier = os.path.join(new_dossier, sousdossier)\n",
    "        os.makedirs(new_sousdossier)\n",
    "print(os.listdir(dossier_racine))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preprocessing_1', 'Test_set_800', 'Train_set_1000', 'Train_set_2000', 'Train_set_400', 'Train_set_4000', 'Train_set_4580']\n",
      "['COVID', 'LUNG OPACITY', 'NORMAL', 'VIRAL PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "#On vérifie\n",
    "print(os.listdir(dossier_racine))\n",
    "print(os.listdir(os.path.join(dossier_racine,\"Train_set_1000\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#à partir du total 1345/classe\n",
    "#On copie 4 jeux :\n",
    "#    - 400 images (100*4)\n",
    "#    - 1000 images (250 * 4)\n",
    "#    - 2000 images (500 * 4)\n",
    "#    - 4000 images (1000 * 4) \n",
    "#    - Toutes les images : 4580 (1145/classe)\n",
    "\n",
    "\n",
    "\n",
    "#Nécessite de calculer l'équilibre des classes en amont\n",
    "def extract_subset(dossier_source, dossier_cible_train, dossier_cible_test, nb_img_train_par_classe:int, nb_img_test_par_classe:int):\n",
    "    #Parcourir les dossiers du dataset préprocessé\n",
    "    for c in os.listdir(dossier_source):\n",
    "        #parcourir chaque classe \n",
    "        classe = os.path.join(dossier_source, c)\n",
    "        #Créer un array randomisé et séparer en TRAIN/TEST d'après le pct de split\n",
    "        #boucle sur un array randomisé entre 1 et le nombre d'images total à extraire pour chaque classe\n",
    "        nb_total_img_a_extraire = nb_img_train_par_classe + nb_img_test_par_classe\n",
    "        random_array = np.random.choice(os.listdir(classe), nb_total_img_a_extraire, replace = False)\n",
    "        #Extraction du jeu de TEST en prenant les X premiers fichiers randomisés\n",
    "        for file in tqdm(random_array[:nb_img_test_par_classe]):\n",
    "            #chemin d'accès au fichier soruce\n",
    "            src = os.path.join(classe, file)\n",
    "            #chemin d'accès du fichier copié\n",
    "            dst = os.path.join(dossier_cible_test, c, file)\n",
    "            #copie\n",
    "            shutil.copyfile(src, dst)\n",
    "        #Extraction du jeu de TRAIN\n",
    "        for file in tqdm(random_array[nb_img_test_par_classe:]):\n",
    "            #chemin d'accès au fichier soruce\n",
    "            src = os.path.join(classe, file)\n",
    "            #chemin d'accès du fichier copié\n",
    "            dst = os.path.join(dossier_cible_train, c, file)\n",
    "            #copie\n",
    "            shutil.copyfile(src, dst)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1004.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 450.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#On fait tourner la fonction pour tester\n",
    "dossier_source = \"../../../Projets data/Projet-radio-DST_TG/data/test2/\" \n",
    "dossier_cible_train = \"../../../Projets data/Projet-radio-DST_TG/data/test3/Train\"\n",
    "dossier_cible_test = \"../../../Projets data/Projet-radio-DST_TG/data/test3/Test\"\n",
    "extract_subset(dossier_source, dossier_cible_train, dossier_cible_test, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/201 [00:00<00:00, 368.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 201/201 [00:00<00:00, 358.80it/s]\n",
      "100%|██████████| 1144/1144 [00:02<00:00, 396.86it/s]\n",
      "100%|██████████| 201/201 [00:00<00:00, 419.73it/s]\n",
      "100%|██████████| 1144/1144 [00:03<00:00, 368.38it/s]\n",
      "100%|██████████| 201/201 [00:00<00:00, 392.35it/s]\n",
      "100%|██████████| 1144/1144 [00:03<00:00, 362.27it/s]\n",
      "100%|██████████| 201/201 [00:00<00:00, 410.08it/s]\n",
      "100%|██████████| 1144/1144 [00:03<00:00, 373.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# D'abord on isole un jeu de test (15% = 201 imgs/classe) et un set d'entraînement \"MAX\" à partir du jeu d'images préprocessé  \n",
    "dossier_source = \"../data/Preprocessing_1/\"\n",
    "dossier_cible_train = \"../data/Train_set_4576/\"\n",
    "dossier_cible_test = \"../data/Test_set_800/\"\n",
    "extract_subset(dossier_source, dossier_cible_train, dossier_cible_test, 1144, 201)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 103.97it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 112.46it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 106.99it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#Ensuite on créée des jeux de test réduits à partir du jeu complet pour tester les différentes test sizes\n",
    "#400\n",
    "for i in [400]:\n",
    "    dossier_source = \"../data/Train_set_4576/\"\n",
    "    dossier_cible_train = \"../data/Train_set_\" + str(i)\n",
    "    dossier_cible_test = \"../data/\"\n",
    "    extract_subset(dossier_source, dossier_cible_train, dossier_cible_test, i//4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 250/250 [00:02<00:00, 98.24it/s] \n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 250/250 [00:03<00:00, 82.15it/s] \n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 250/250 [00:02<00:00, 104.04it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 250/250 [00:02<00:00, 90.00it/s] \n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 80.89it/s] \n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 69.17it/s] \n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 108.96it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 110.39it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 181.36it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 177.04it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 178.32it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1000/1000 [00:06<00:00, 148.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in [1000, 2000, 4000]:\n",
    "    dossier_source = \"../data/Train_set_4576/\"\n",
    "    dossier_cible_train = \"../data/Train_set_\" + str(i)\n",
    "    dossier_cible_test = \"../data/\"\n",
    "    extract_subset(dossier_source, dossier_cible_train, dossier_cible_test, i//4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tous les jeux sont bons pour les prochains modèms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
