{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/gpenessot/OCT23_BDS_Radios_Poumons/blob/main/4_Transfer_learning_%26_hyperparameter_tuning_VGG19_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"KsVOSiRonfyu"},"source":["# Projet DST - Etape 3 Modélisation - Transfer learning\n","\n","Notebook adapté du notebook commun \"baseline - hyperparameter tuning\""]},{"cell_type":"markdown","metadata":{"id":"1Nn1KU5ToKmr"},"source":["## 1) Montage Drive & import librairies : à faire à chaque ouverture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCs9-rvGiO2S"},"outputs":[],"source":["# Connection du notebook au google drive perso\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qOirs8maDGV"},"outputs":[],"source":["# Installation du keras tuner dans l'environnement du notebook car pas installé par défaut\n","\n","!pip install keras-tuner -q\n","import keras_tuner\n","from keras_tuner.tuners import  Hyperband"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UezVN50bLejO"},"outputs":[],"source":["# Import des packages / librairies / fonctions nécessaires\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import PIL\n","import tensorflow as tf\n","import cv2\n","import zipfile\n","import pathlib\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","\n","\n","\n","from keras.callbacks import EarlyStopping\n","\n","#Ajouts Thibaut :\n","import pickle #pour sauvegarde de l'historique d'entraînement\n","import os\n","import pandas as pd\n","#pour le Grad-CAM\n","from IPython.display import Image, display\n","import matplotlib\n","import matplotlib.cm as cm"]},{"cell_type":"markdown","metadata":{"id":"KPPcTNBEoGdU"},"source":["## 2) Dézippage sets d'images : fait 1 fois"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yN63TIU9-ZZQ"},"outputs":[],"source":["# Dézippage des sets d'images (stockés sur google drive) dans l'environnement du notebook\n","\n","# Dézippage train set\n","!unzip -q '/content/drive/MyDrive/Colab/TrainTest.zip'"]},{"cell_type":"markdown","metadata":{"id":"9-wPnMS_oVaP"},"source":["## 3) Import des datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WB9IhG-uQKw"},"outputs":[],"source":["#A MODIFIER PAR L'UTILISATEUR : Paramétrage des dossiers locaux pour les datasets et les sauvegardes :\n","\n","# Chemin du dossier TRAIN dézippé : à adapter si stocké ailleurs\n","dossier_train = \"/content/Train/\"\n","\n","# Chemin du dossier TEST dézippé : à adapter si stocké ailleurs\n","dossier_test = \"/content/Test\"\n","\n","#Dossier de sauvegarde du modèle au format.keras et de l'historique d'entraînement au format pickle\n","dossier_sauv = \"/content\"\n","\n","#Nom du dataset (arbitraire), pour intégrer dans le nom de la sauvegarde et s'y retrouver plus facilement\n","#N'a pas de rapport avec le nom du dossier\n","nom_dataset = \"TrainTest\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOQpxF0HHaFM"},"outputs":[],"source":["# Affichage du lien du répertoire du set d'entrainement\n","train_set_dir = pathlib.Path(dossier_train).with_suffix('')\n","\n","# Vérification nombre d'images dans le set d'entrainement\n","print (\"Répertoire train set : \", train_set_dir)\n","train_set_image_count = len(list(train_set_dir.glob('*/*.png')))\n","print(\"Nombre images train set \", train_set_image_count)\n","\n","print (\"\\n\")\n","\n","# Affichage du lien du répertoire du set de test\n","test_set_dir = pathlib.Path(dossier_test).with_suffix('')\n","\n","# Vérification nombre d'images dans le set de test\n","print (\"Répertoire test set : \", test_set_dir)\n","test_set_image_count = len(list(test_set_dir.glob('*/*.png')))\n","print(\"Nombre images test set \", test_set_image_count)"]},{"cell_type":"markdown","metadata":{"id":"Tr9KKqnT_RVx"},"source":["## 3) Paramètres globaux"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIjqU9XF31Gy"},"outputs":[],"source":["IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","MODE = 'rgb'   # rgb or grayscale\n","\n","AUGMENTATION_ON = False\n","\n","NUM_EPOCHS = 50\n","BATCH_SIZE = 32\n","VALIDATION_SPLIT = 0.2\n","L_RATE = 0.001\n","\n","NAME_TO_SAVE = \"Resnet152v2_unfrozen\"\n","\n","keras_tuner_logs_dir = \"keras_tuner/logs\"\n","\n","CALLBACKS = [\n","    tf.keras.callbacks.ModelCheckpoint(NAME_TO_SAVE+'.weights.h5', save_best_only=True,save_weights_only=True, monitor=\"val_accuracy\"),\n","    tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_accuracy\", factor=0.2, patience=6, min_lr=0.000000001),\n","    tf.keras.callbacks.TensorBoard('keras_tuner/log')]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKC3uFNqNor-"},"outputs":[],"source":["# Création des sets d'entrainement et de validation au format keras\n","# Attention : modèles de transfer learning requièrent du RGB en input\n","\n","# Création du set d'entrainement\n","train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n","  train_set_dir,\n","  validation_split=VALIDATION_SPLIT,\n","  subset=\"both\",\n","  seed=123,\n","  image_size=(IMG_HEIGHT, IMG_WIDTH),\n","  color_mode=MODE,\n"," # color_mode = \"rgb\", #pour les modèles de transfer_learning\n","  batch_size=BATCH_SIZE,\n","  label_mode= 'categorical')\n","\n","\n","# Création du set de validation\n","test_ds = tf.keras.utils.image_dataset_from_directory(\n","  train_set_dir,\n","  seed=123,\n","  image_size=(IMG_HEIGHT, IMG_WIDTH),\n","  color_mode=MODE,\n","#  color_mode = \"rgb\", #pour les modèles de transfer_learning\n","  batch_size=BATCH_SIZE,\n","  label_mode= 'categorical')\n","\n","    #Configure dataset for performance with CACHE (not loading images at each epoch)\n","    #and PREFETCH (overlapping data processing and model execution)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wVNXYNnvsuKa"},"source":["## 4) Transfer learning sans tuning"]},{"cell_type":"markdown","metadata":{"id":"SVmuywEpqsz4"},"source":["### Import d'un modèle pré-entraîné pour le transfer learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbjV74nUP3z6"},"outputs":[],"source":["#Import du modèle pré-entraîné et des poids sur un jeu de données\n","from tensorflow.keras.applications.resnet_v2 import ResNet152V2, preprocess_input, decode_predictions\n","\n","#Attention, l'input standard de VGG16 est 224*224*3, il faudra ajouter une couche de resizing\n","#img_size_dataset = (img_height,img_width)\n","#img_size_model = (224, 224)\n","\n","# Modèle VGG16 sans les couches denses de classification. Nécessite input en RGB\n","base_model = ResNet152V2(include_top=True,\n","    weights='imagenet',\n","    input_tensor=None,\n","    input_shape=None,\n","    pooling=None,\n","    classes=1000,\n","    classifier_activation=None)\n","\n","# Freezer les couches du VGG16 pour ne pas les ré-entraîner\n","for layer in base_model.layers:\n","    layer.trainable = True\n","\n","#Appliquer le preprocessing sur les inputs (\"Note: each Keras Application expects a specific kind of input preprocessing. resnet_v2.preprocess_input will scale input pixels between -1 and 1\n","#La fonction preprocess_input est dépendante du modèle choisi (à importer avec le modèle en début de cellule)\n","def preprocess(images, labels):\n","  return preprocess_input(images), labels\n","\n","train_ds = train_ds.map(preprocess)\n","val_ds = val_ds.map(preprocess)\n","#Attention, le préprocess réécrit sur train_ds et val_ds, il faut donc le réimporter si on change de modèle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2g07FFjFrDQ-"},"outputs":[],"source":["inputs = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n","x = keras.layers.Rescaling(1./255)(inputs)\n","x = base_model(x)\n","x = layers.Flatten()(x)\n","x = layers.Dense(units=128, activation=\"tanh\")(x)\n","x = layers.Dense(units=32, activation=\"tanh\")(x)\n","x = layers.Dropout(0.2)(x)\n","x = layers.Dense(units=16, activation=\"tanh\")(x)\n","x = layers.Dense(4, activation = \"softmax\")(x)\n","\n","\n","model = keras.Model(inputs, x)\n","model._name = NAME_TO_SAVE\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uJukwE-qrWl1"},"source":["### Compilation et entraînement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2j9uUtFdrDbg"},"outputs":[],"source":["#Compilation\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=[keras.metrics.CategoricalAccuracy(name ='accuracy')])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MzSBXNkrDe3"},"outputs":[],"source":["# Définition d'une callback qui stoppe le processus d'entrainement quand une valeur monitorée\n","# (ici la validation loss) stagne pendant un certain nombre d'epochs (et ce à partir d'un certain nombre d'epochs)\n","\n","CALLBACKS = [\n","    tf.keras.callbacks.ModelCheckpoint(NAME_TO_SAVE+'.weights.h5', save_best_only=True,save_weights_only=True, monitor=\"val_accuracy\"),\n","    tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_accuracy\", factor=0.2, patience=6, min_lr=0.000000001),\n","    tf.keras.callbacks.TensorBoard(\"keras_tuner/logs\")]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Jkq_CFVJrDiG"},"outputs":[],"source":["#Entraînement\n","history = model.fit(train_ds,\n","                    validation_data=val_ds,\n","                    epochs=NUM_EPOCHS,\n","                    callbacks = CALLBACKS )\n"]},{"cell_type":"markdown","metadata":{"id":"DH8_JlofQNFC"},"source":["###"]},{"cell_type":"markdown","metadata":{"id":"Y9OmKZTwQPkF"},"source":["### Affichage des résultats et Enregistrement du modèle et des résultats"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oCs2x8i_xI5y"},"outputs":[],"source":["def plot_history_metrics(history,name_to_save):\n"," acc = history.history['accuracy']\n"," val_acc = history.history['val_accuracy']\n","\n"," loss = history.history['loss']\n"," val_loss = history.history['val_loss']\n","\n","\n"," fig = plt.figure(figsize=(12,6))\n"," ax1 = fig.add_subplot(121)\n","\n"," ax1.plot(acc, label='Training Accuracy')\n"," ax1.plot(val_acc, label='Validation Accuracy')\n"," ax1.legend(loc='lower right')\n"," ax1.set_title(name_to_save + 'Accuracy')\n","\n"," ax2 = fig.add_subplot(122)\n"," ax2.plot(loss, label='Training Loss')\n"," ax2.plot(val_loss, label='Validation Loss')\n"," ax2.legend(loc='upper right')\n"," ax2.set_title( name_to_save + 'Loss')\n"," plt.show()\n","\n"," fig.savefig('T&V' + name_to_save + '.png')\n"," plt.close(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oYYNtOgOxp9D"},"outputs":[],"source":["#Sauvegarde du modèle et des résultats\n","\n","#Sauvegarde du modèle\n","val_acc_per_epoch = history.history['val_accuracy']\n","best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n","\n","model.save('/content/'+ NAME_TO_SAVE + '.keras')\n","\n","#Export results to dict & csv\n","\n","#Dump to a pickle file\n","# Pickle the history to file\n","filepath = f\"{NAME_TO_SAVE}_history.pkl\"\n","filepath = os.path.join(dossier_sauv, filepath)\n","with open(filepath, 'wb') as f:\n","    pickle.dump(history, f)\n","\n","#Impression du schéma\n","plot_history_metrics(history, NAME_TO_SAVE )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Kunqzgvk5Z7P"},"outputs":[],"source":["#Métriques sur jen d'entraînement\n","best_val_loss_index = np.argmin(history.history[\"val_loss\"])\n","print(\"Best validation loss :\", round(np.max(history.history[\"val_loss\"]), 2) )\n","print(\"Validation ACCURACY : \", round(history.history[\"val_accuracy\"][best_val_loss_index], 2))\n","print(\"Obtained at epoch : \", best_val_loss_index + 1)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sMhxLZUjhHMn"},"source":["## 6) Evaluer le modèle sur un jeu de test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Wsz7vlIBhHMk"},"outputs":[],"source":["model.load_weights(NAME_TO_SAVE+'.weights.h5')\n","\n","accuracy = model.evaluate(test_ds, return_dict=True)\n","\n","print('TEST ACCURACY:',accuracy.get('accuracy'))"]},{"cell_type":"markdown","metadata":{"id":"WkHQ6fsZQBZz"},"source":["## 7) Interprétabilité Grad-CAM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yLy2HEywGrK"},"outputs":[],"source":["from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input, decode_predictions\n"]},{"cell_type":"markdown","metadata":{"id":"DWiu9peheDRA"},"source":["\n","### Paramètres à configurer selon le modèle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"odqWjskhWhxU"},"outputs":[],"source":["img_size = (256, 256)\n","\n","\n","model.layers[4].activation = None\n","\n","last_conv_layer_name = \"conv2d_7\"\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w7WqihtTWvLC"},"source":["### Définition des fonctions\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0ITxCr3kcMD"},"outputs":[],"source":["#Fonction pour charger une nouvelle image comme array pour injecter dans la fonction GradCam\n","def get_img_array(img_path, size):\n","    # `img` is a PIL image of size 256*256\n","    img = keras.utils.load_img(img_path, target_size=size)\n","    # `array` is a float32 Numpy array of shape (256, 256, 3)\n","    array = keras.utils.img_to_array(img)\n","    # We add a dimension to transform our array into a \"batch\"\n","    # of size (1, 256,256, 3)\n","    array = np.expand_dims(array, axis=0)\n","    return array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBJ0zHtJj4E-"},"outputs":[],"source":["#Fonction grad_cam\n","def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer as well as the output predictions\n","    grad_model = keras.models.Model(\n","        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n","    )\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    # This is the gradient of the output neuron (top predicted or chosen)\n","    # with regard to the output feature map of the last conv layer\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    # then sum all the channels to obtain the heatmap class activation\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","    return heatmap.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Vr6-PeNWBRd"},"outputs":[],"source":["def create_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.5):\n","    # Load the original image\n","    img = keras.utils.load_img(img_path)\n","    img = keras.utils.img_to_array(img)\n","\n","    # Rescale heatmap to a range 0-255\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # Use jet colormap to colorize heatmap\n","    jet = matplotlib.colormaps[\"jet\"]\n","\n","    # Use RGB values of the colormap\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","\n","    # Create an image with RGB colorized heatmap\n","    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n","    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n","    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n","\n","    # Superimpose the heatmap on original image\n","    superimposed_img = jet_heatmap * alpha + img\n","    superimposed_img = keras.utils.array_to_img(superimposed_img)\n","\n","    # Save the superimposed image\n","    superimposed_img.save(cam_path)\n","\n","    # Display Grad CAM\n","    #display(Image(img_path))\n","\n","    #Return superimposed_img\n","    return superimposed_img\n"]},{"cell_type":"markdown","metadata":{"id":"M3e96wOeW4b5"},"source":["### Test sur 1 image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vjfg4PZR10u7"},"outputs":[],"source":["#Création de la heatmap pour l'image donnée, en utilisant la dernière couche de convolution du modèle\n","\n","#Tirer et affichier une image au hasard dans le dataset Train\n","train_set_list = list(train_set_dir.glob('*/*.png'))\n","k = int(np.random.choice(len(train_set_list), 1))\n","img_path = str(train_set_list[k])\n","img = keras.utils.load_img(img_path)\n","\n","#Récupérer le nom de l'image pour afficher la vraie classe\n","img_name = str(img_path).rsplit(\"/\",1)[1]\n","real_class_name = img_name.split(\"-\")[0]\n","\n","# Prepare image\n","img_array = preprocess_input(get_img_array(img_path, size=(IMG_HEIGHT,IMG_WIDTH)))\n","\n","# Print what the top predicted class is\n","preds = model.predict(img_array)\n","\n","#Si le modèle est chargé avec sa couche de classification, utiliser la fonction standard de Keras :\n","#prediction = decode_predictions(preds, top=1)[0]\n","\n","#Sinon, le nombre de classes est différent, on calcule à la main:\n","best_pred_index = preds.argmax()\n","prediction = train_ds.class_names[best_pred_index]\n","\n","print(img_path)\n","print(\"Classe réelle : \", real_class_name)\n","print(\"Prédiction : \", prediction)\n","\n","# Generate class activation heatmap\n","heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n","\n","\n","# Display heatmap\n","plt.matshow(heatmap)\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QG5hX2qLaIz_"},"outputs":[],"source":["#Afficher les images superposées ainsi que la prédiction\n","\n","#Création de la gradcam : fonctions adaptées depuis https://keras.io/examples/vision/grad_cam/\n","gradcam = create_gradcam(img_path, heatmap)\n","\n","plt.figure(figsize = (10, 5))\n","\n","#Affichage de la classe réelle et prédite\n","print(\"Classe réelle : \",real_class_name)\n","#fonction pour décoder la prédiction à adapter\n","print(\"Classe prédite par le modèle : \", prediction)\n","\n","\n","#Affichage de l'image source\n","plt.subplot(121)\n","plt.imshow(img, cmap = \"gray\")\n","plt.title(img_name)\n","plt.axis(\"off\")\n","\n","#AFfichage de la grad-cam\n","plt.subplot(122)\n","plt.imshow(gradcam)\n","plt.axis(\"off\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-IFa2uoAu7d"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1ApBmQ4OA7yBONqqjHsFe5b2ryMxUtSq-","timestamp":1702052515330},{"file_id":"1zFoKl-pMWLeB_Q1Rs2p8H4DKSYa1bKBT","timestamp":1702043246971}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}